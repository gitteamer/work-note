[TOC]



## 开发设计规范



### 协调规则

1. **修改CR的Spec、Status、metav1.TypeMeta、metav1.ObjectMeta都会触发控制器协调。**
2. **无论是用户修改，还是CR控制器去修改，都会触发协调。**
3. **修改CR的Spec、metav1.TypeMeta、metav1.ObjectMeta：metav1.ObjectMeta.Generation会+1。**
4. **修改CR的Status：metav1.ObjectMeta.Generation不会改变。**





### Scheme注册

查看main.go文件

```go
import (
	bucketv1 "bucket/api/v1"
    cronjobv1 "cronjob/api/v1"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	// +kubebuilder:scaffold:imports
)

func init() {
	_ = clientgoscheme.AddToScheme(scheme)
	_ = bucketv1.AddToScheme(scheme)
	
	// +kubebuilder:scaffold:scheme
    _ = cronjobv1.AddToScheme(scheme)
}
```

> clientgoscheme注册了scheme。注册了k8s系统默认的scheme。
>
> bucketv1注册了当前CRD对应的scheme。要监控该CRD，则需要注册对应的scheme。
>
> 如果还监控了其他自定义的CRD，则也需要再这里注册scheme。例如：cronjobv1.AddToScheme(scheme)注册scheme。





### 更新CR的Status

> 设计subresource风格的status

k8s中`status subresource`的使用规范：

1. 用户只能指定一个CR实例的spec部分。

2. CR实例的status部分由控制器进行变更。



- 需要在Bucket的注释中添加一行`// +kubebuilder:subresource:status`，变成如下：

  ```go
  // +kubebuilder:subresource:status
  // +kubebuilder:object:root=true
  
  // Bucket is the Schema for the buckets API
  type Bucket struct {
      metav1.TypeMeta   `json:",inline"`
      metav1.ObjectMeta `json:"metadata,omitempty"`
  
      Spec   BucketSpec   `json:"spec,omitempty"`
      Status BucketStatus `json:"status,omitempty"`
  }
  ```

  如果没有添加上面的注释。则我们在controller的代码中调用到`Status().Update()`,会触发panic，并报错：`the server could not find the requested resource`

- 创建Bucket资源时，即便我们填入了非空的`status`结构，也不会更新到apiserver中。Status只能通过对应的client进行更新。比如在controller中：

  ```go
  if bucket.Status.Progress == 0 {
        bucket.Status.Progress = 1
        err := r.Status().Update(ctx, &bucket)
        if err != nil {
            return ctrl.Result{}, err
        }
  }
  ```

  这样，只要bucket实例的`status.Progress`为0时（比如我们创建一个bucket实例时，由于`status.Progress`无法配置，故初始化为默认值，即0），controller就会帮我们将它变更为1.

  

  注意：

  - kubebuilder 2.0开发生成的crd模板，无法通过apiserver的crd校验。社区有相关的记录和修复

  - 所以1.11.*版本的k8s，要使用kubebuilder 2.0 必须给apiserver配置一个featuregate： - --feature-gates=CustomResourceValidation=false，关闭对crd的校验。



### CR的Conditions

**condition的定义**

```go
import (
	corev1 "k8s.io/api/core/v1"
	v1alpha3 "sigs.k8s.io/cluster-api/api/v1alpha3"
)

// 添加condition的枚举
const (
	ConditionCreate v1alpha3.ConditionType = "Create"
	ConditionReady  v1alpha3.ConditionType = "Ready"
)

type BucketStatus struct {
	// 总的状态，便于直接判断
	State corev1.ConditionStatus `json:"state,omitempty"`

	// 记录Spec的版本号，便于判断是否修改了Spec
	Generation int64  `json:"generation,omitempty"`

	// 每一个操作流程对应的Condition状态
	Conditions v1alpha3.Conditions `json:"conditions,omitempty"`
}
```

**condition的操作**

```go
import (
	"context"
	"fmt"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"time"

	"github.com/go-logr/logr"
	"k8s.io/apimachinery/pkg/runtime"
	bucketv1 "bucket/api/v1"
	"sigs.k8s.io/cluster-api/api/v1alpha3"
	"sigs.k8s.io/cluster-api/util/conditions"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func (r *BucketReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    ctx := context.Background()
	log := r.Log.WithValues("bucket", req.NamespacedName)
    
    // 获取对象
    var bucket Bucketv1.Bucket
	err := r.Get(context.Background(), req.NamespacedName, &bucket)
	if err != nil {
		log.Error(err, "获取bucket失败")
		return ctrl.Result{}, nil
	}
    
    // Status变化, Generation不会改变
	// Spec变化，Generation会+1
    // 获取本次spec的版本与status中存的，进行比较，判断是否需要协调
    if bucket.Status.Generation == bucket.ObjectMeta.Generation {
        log.Info("不需要协调")
        return ctrl.Result{}, nil
    } else {
        log.Info("开始协调")
    }

  	// 相应的业务逻辑操作
    
    // 完成后，更新相应的conditon状态
    conditions.MarkTrue(&bucket, tcpv1.ConditionCreate)
    
    // 更新总的状态
	bucket.Status.State = corev1.ConditionTrue

	// 将Spec的版本存到Status里面
	// 记录本次协调的spec变更的版本
    bucket.Status.Generation = bucket.ObjectMeta.Generation

	// 更新status
    err = r.Status().Update(ctx, &bucket)
    if err != nil {
        log.Error(err, "更新bucket失败")
    }
    
    return ctrl.Result{}, nil
}
```

> Generation只能放在Status中，不能放在Annotation里面。
>
> 如果放在Annotation里面，更新Annotation，Generation会+1，会额外比放在Status里面多一次协调。并且更新的时候只能使用r.Update更新全部数据。如果使用r.Status().Update则只能更新Status的数据，而不能更新Spec、metav1.TypeMeta、metav1.ObjectMeta的数据。
>
> 使用r.Update更新数据，metav1.ObjectMeta.Generation会+1，会额外多一次协调。



### 资源命令行显示更多列

> 允许在`kubectl get`命令中显示crd的更多字段

kubectl get 时显示crd的status.replicas：

```shell
// +kubebuilder:printcolumn:JSONPath=".status.replicas",name=Replicas,type=string
```

添加后，使用kubectl get，即可以显示出对应的Replicas列的数据

```
type CronjonStatus struct {
   State    string `json:"state,omitempty"`
   Replicas string `json:"replicas,omitempty"`
}

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:printcolumn:JSONPath=".status.replicas",name=Replicas,type=string
// Cronjob is the Schema for the Cronjobs API
type Cronjob struct {
   metav1.TypeMeta   `json:",inline"`
   metav1.ObjectMeta `json:"metadata,omitempty"`

   Spec   CronjobSpec   `json:"spec,omitempty"`
   Status CronjobStatus `json:"status,omitempty"`
}
```



### Finalizers删除资源

[Finalizers删除规则](Finalizers删除规则.md)





### reconciler监控多个资源变化

我们在开发过程中，可能需要开发一个类似`service-->selector-->pods`的资源逻辑，那么，在service的reconciler里，我们关注service的seletor的配置，并且检查匹配的pods是否有所变更（增加或减少），并更新到同名的endpoints里；同时，我们还要关注pod的更新，如果pod的label发生变化，那么要找出所有'之前匹配了这些pod'的service，检查service的selector是否仍然匹配pod的label，如有变动，也要更新endpoints。

这就意味着，我们需要能让reconciler能观察到service和pod两种资源的变更。我们在serviceReconciler的SetupWithManager方法中，可以看到：

```reasonml
import 	v1 "k8s.io/api/core/v1"

func (r *ServiceReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&v1.Service{}).
        Complete(r)
}
```



#### Watches方式

只需要在`For`方法调用后再调用`Watches`方法即可:

```go
import (
	corev1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/source"
)

func (r *ServiceReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&corev1.Service{}).
        // 监控其他资源，例如：Pod
        Watches(&source.Kind{Type: &corev1.Pod{}}, &handler.EnqueueRequestForObject{}).
        Complete(r)
}
```



#### Owns方式

此外，我们可以将service设计为pod的owner，然后在ServiceReconciler的`For`方法后在调用`Owns`方法：

```go
func (r *ServiceReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&corev1.Service{}).
        // 监控的资源有owner关系，等效于：Owns(&corev1.Pod{})
		//Watches(&source.Kind{Type: &corev1.Pod{}}, &handler.EnqueueRequestForOwner{OwnerType: &corev1.Service{}, IsController: true}).
    	// 将Service设计为pod的owner，则只有属于owner关系的pod的create/delete/update事件，才会触发owner对象的Reconcile调谐
        Owns(&corev1.Pod{}).
        Complete(r)
}
```

> 只有属于owner关系的pod的create/delete/update事件，才会触发owner对象的Reconcile调谐



我们在`Owns`方法的定义注释中可以看到它与Watch方法其实是类似的：

```go
// Owns defines types of Objects being *generated* by the ControllerManagedBy, and configures the ControllerManagedBy to respond to
// create / delete / update events by *reconciling the owner object*.  This is the equivalent of calling
// Watches(&handler.EnqueueRequestForOwner{&source.Kind{Type: <ForType-apiType>}, &handler.EnqueueRequestForOwner{OwnerType: apiType, IsController: true})
func (blder *Builder) Owns(apiType runtime.Object) *Builder {
    blder.managedObjects = append(blder.managedObjects, apiType)
    return blder
}
```

> Owns(&v1.Pod{})
>
> 方法等效于：Watches(&source.Kind{Type: <ForType-apiType>}, &handler.EnqueueRequestForOwner{OwnerType: apiType, IsController: true})
>
> 即：Watches(&source.Kind{Type: &v1.Pod{}}, &handler.EnqueueRequestForOwner{OwnerType: &v1.Service{}, IsController: true})

不论是`For`,`Own`,`Watch`,都是kubebuilder中的`Builder`提供的，`Builder`是kubebuilder开放给用户构建控制器的唯一合法入口（你还可以用更hack的手段去构建，可能对源码造成入侵），它还提供了许多有用的方法，可以让我们更灵活自由地初始化一个controller。

**注意：kubebuilder 2.0中，构建一个reconciler时，可以用`Own`,`Watch`方法来额外监听一些资源，但是`For`方法必须要有，如果没有`For`方法，编译出来的程序运行时会报错，类似于"kind.Type should not be empty"**







### 使用索引

client-go支持构建带索引的缓存，并且允许用户自定义索引函数的名称和内容。当缓存的数据带有索引时，我们可以更快地通过索引List到想要的数据，既可以提高性能又可以减少代码量。

kubebuilder 2.0 提供了很简单的索引构建方式。 比如我们要以pod中的spec.NodeName为索引， 方便我们快速List查询某个节点上的pod：

```stylus
// main函数中添加
// 增加索引：设置索引名称，和对象的回调函数。
_ = mgr.GetFieldIndexer().IndexField(&v1.Pod{}, "indexNodeNameOfPod", func(o runtime.Object) []string {
    v := o.(*v1.Pod).Spec.NodeName
    return []string{v}
})

// 控制器的Reconcile调谐函数中使用，或者main函数的mgr.GetClient()使用。
// 使用manager的client进行List
podList := &v1.PodList{}
err := mgr.GetClient().List(context.TODO(), podList, &client.MatchingFields{"indexNodeNameOfPod": node.Name})
```

