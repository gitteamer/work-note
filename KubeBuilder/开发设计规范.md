[TOC]



## 开发设计规范



### 更新CRD的Status

> 设计subresource风格的status

k8s中`status subresource`的使用规范：

1. 用户只能指定一个CRD实例的spec部分。

2. CRD实例的status部分由控制器进行变更。



- 需要在Bucket的注释中添加一行`// +kubebuilder:subresource:status`，变成如下：

  ```go
  // +kubebuilder:subresource:status
  // +kubebuilder:object:root=true
  
  // Bucket is the Schema for the buckets API
  type Bucket struct {
      metav1.TypeMeta   `json:",inline"`
      metav1.ObjectMeta `json:"metadata,omitempty"`
  
      Spec   BucketSpec   `json:"spec,omitempty"`
      Status BucketStatus `json:"status,omitempty"`
  }
  ```

  如果没有添加上面的注释。则我们在controller的代码中调用到`Status().Update()`,会触发panic，并报错：`the server could not find the requested resource`

- 创建Bucket资源时，即便我们填入了非空的`status`结构，也不会更新到apiserver中。Status只能通过对应的client进行更新。比如在controller中：

  ```go
  if bucket.Status.Progress == 0 {
        bucket.Status.Progress = 1
        err := r.Status().Update(ctx, &bucket)
        if err != nil {
            return ctrl.Result{}, err
        }
  }
  ```

  这样，只要bucket实例的`status.Progress`为0时（比如我们创建一个bucket实例时，由于`status.Progress`无法配置，故初始化为默认值，即0），controller就会帮我们将它变更为1.

  

  注意：

  - kubebuilder 2.0开发生成的crd模板，无法通过apiserver的crd校验。社区有相关的记录和修复

  - 所以1.11.*版本的k8s，要使用kubebuilder 2.0 必须给apiserver配置一个featuregate： - --feature-gates=CustomResourceValidation=false，关闭对crd的校验。



### CRD的Conditions

**condition的定义**

```go
import (
	corev1 "k8s.io/api/core/v1"
	v1alpha3 "sigs.k8s.io/cluster-api/api/v1alpha3"
)

// 添加condition的枚举
const (
	ConditionCreate v1alpha3.ConditionType = "Create"
	ConditionReady  v1alpha3.ConditionType = "Ready"
)

type BucketStatus struct {
	// 总的状态，便于直接判断
	State corev1.ConditionStatus `json:"state,omitempty"`

	// 记录Spec的版本号，便于判断是否修改了Spec
	Generation int64  `json:"generation,omitempty"`

	// 每一个操作流程对应的Condition状态
	Conditions v1alpha3.Conditions `json:"conditions,omitempty"`
}
```

**condition的操作**

```go
import (
	"context"
	"fmt"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"time"

	"github.com/go-logr/logr"
	"k8s.io/apimachinery/pkg/runtime"
	bucketv1 "Bucket/api/v1"
	"sigs.k8s.io/cluster-api/api/v1alpha3"
	"sigs.k8s.io/cluster-api/util/conditions"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func (r *BucketReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    ctx := context.Background()
	log := r.Log.WithValues("route", req.NamespacedName)
    
    // 获取对象
    var bucket Bucketv1.Bucket
	err := r.Get(context.Background(), req.NamespacedName, &bucket)
	if err != nil {
		log.Error(err, "获取bucket失败")
		return ctrl.Result{}, nil
	}
    
    // Status变化, Generation不会改变
	// Spec变化，Generation会+1
    // 获取本次spec的版本与status中存的，进行比较，判断是否需要协调
    if bucket.Status.Generation == bucket.ObjectMeta.Generation {
        log.Info("不需要协调")
        return ctrl.Result{}, nil
    } else {
        log.Info("开始协调")
    }

  	// 相应的业务逻辑操作
    
    // 完成后，更新相应的conditon状态
    conditions.MarkTrue(&bucket, tcpv1.ConditionCreate)
    
    // 更新总的状态
	bucket.Status.State = corev1.ConditionTrue

	// 将Spec的版本存到Status里面
	// 记录本次协调的spec变更的版本
    bucket.Status.Generation = bucket.ObjectMeta.Generation

	// 更新status
    err = r.Status().Update(ctx, &bucket)
    if err != nil {
        log.Error(err, "更新bucket失败")
    }
    
    return ctrl.Result{}, nil
}
```

> Generation只能放在Status中，不能放在Annotation里面。
>
> 如果放在Annotation里面，更新的时候不能只更新status的数据r.Status().Update。需要更新全部数据r.Update。那么会额外触发一次协调。





### 资源命令行显示更多列

> 允许在`kubectl get`命令中显示crd的更多字段

kubectl get 时显示crd的status.replicas：

```shell
// +kubebuilder:printcolumn:JSONPath=".status.replicas",name=Replicas,type=string
```

添加后，使用kubectl get，即可以显示出对应的Replicas列的数据

```
type CronjonStatus struct {
   State    string `json:"state,omitempty"`
   Replicas string `json:"replicas,omitempty"`
}

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:printcolumn:JSONPath=".status.replicas",name=Replicas,type=string
// Cronjob is the Schema for the Cronjobs API
type Cronjob struct {
   metav1.TypeMeta   `json:",inline"`
   metav1.ObjectMeta `json:"metadata,omitempty"`

   Spec   CronjobSpec   `json:"spec,omitempty"`
   Status CronjobStatus `json:"status,omitempty"`
}
```



### finalizer

`finalizer`即终结器，存在于每一个k8s内的资源实例中，即`**.metadata.finalizers`,它是一个字符串数组，每一个成员表示一个`finalizer`。控制器在删除某个资源时，会根据该资源的`finalizers`配置，进行异步预删除处理，所有的`finalizer`都执行完毕后，该资源会被真正删除。

这里的预删除处理，一般指对该资源的关联资源进行增删改操作。比如：一个A资源被删除时，其finalizer规定必须将A资源的Selector指向的所有service都删除。

当我们需要设计这类finalizer时，就可以自定义一个controller来实现。



### reconciler监控多个资源变化

我们在开发过程中，可能需要开发一个类似`service-->selector-->pods`的资源逻辑，那么，在service的reconciler里，我们关注service的seletor的配置，并且检查匹配的pods是否有所变更（增加或减少），并更新到同名的endpoints里；同时，我们还要关注pod的更新，如果pod的label发生变化，那么要找出所有'之前匹配了这些pod'的service，检查service的selector是否仍然匹配pod的label，如有变动，也要更新endpoints。

这就意味着，我们需要能让reconciler能观察到service和pod两种资源的变更。我们在serviceReconciler的SetupWithManager方法中，可以看到：

```reasonml
import 	v1 "k8s.io/api/core/v1"

func (r *ServiceReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&v1.Service{}).
        Complete(r)
}
```



只需要在`For`方法调用后再调用`Watches`方法即可:

```reasonml
import (
	v1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/source"
)

func (r *ServiceReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&opsv1.Service{}).
        Watches(&source.Kind{Type: &opsv1.Pod{}}, &handler.EnqueueRequestForObject{}).
        Complete(r)
}
```



此外，我们可以将service设计为pod的owner，然后在ServiceReconciler的`For`方法后在调用`Owns`方法：

```reasonml
func (r *ServiceReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&v1.Service{}).
        Owns(&v1.Pod{}).
        Complete(r)
}
```

> 只有属于owner关系的pod的create/delete/update事件，才会触发owner对象的Reconcile调谐



我们在`Owns`方法的定义注释中可以看到它与Watch方法其实是类似的：

```pgsql
// Owns defines types of Objects being *generated* by the ControllerManagedBy, and configures the ControllerManagedBy to respond to
// create / delete / update events by *reconciling the owner object*.  This is the equivalent of calling
// Watches(&handler.EnqueueRequestForOwner{&source.Kind{Type: <ForType-apiType>}, &handler.EnqueueRequestForOwner{OwnerType: apiType, IsController: true})
func (blder *Builder) Owns(apiType runtime.Object) *Builder {
    blder.managedObjects = append(blder.managedObjects, apiType)
    return blder
}
```

> Owns(&v1.Pod{})
>
> 方法等效于：Watches(&source.Kind{Type: <ForType-apiType>}, &handler.EnqueueRequestForOwner{OwnerType: apiType, IsController: true})
>
> 即：Watches(&source.Kind{Type: &v1.Pod{}}, &handler.EnqueueRequestForOwner{OwnerType: &v1.Service{}, IsController: true})

不论是`For`,`Own`,`Watch`,都是kubebuilder中的`Builder`提供的，`Builder`是kubebuilder开放给用户构建控制器的唯一合法入口（你还可以用更hack的手段去构建，可能对源码造成入侵），它还提供了许多有用的方法，可以让我们更灵活自由地初始化一个controller。

**注意：kubebuilder 2.0中，构建一个reconciler时，可以用`Own`,`Watch`方法来额外监听一些资源，但是`For`方法必须要有，如果没有`For`方法，编译出来的程序运行时会报错，类似于"kind.Type should not be empty"**





### 使用索引

client-go支持构建带索引的缓存，并且允许用户自定义索引函数的名称和内容。当缓存的数据带有索引时，我们可以更快地通过索引List到想要的数据，既可以提高性能又可以减少代码量。

kubebuilder 2.0 提供了很简单的索引构建方式。 比如我们要以pod中的spec.NodeName为索引， 方便我们快速List查询某个节点上的pod：

```stylus
// main函数中添加
// 增加索引：设置索引名称，和对象的回调函数。
_ = mgr.GetFieldIndexer().IndexField(&v1.Pod{}, "indexNodeNameOfPod", func(o runtime.Object) []string {
    v := o.(*v1.Pod).Spec.NodeName
    return []string{v}
})

// 控制器的Reconcile调谐函数中使用，或者main函数的mgr.GetClient()使用。
// 使用manager的client进行List
podList := &v1.PodList{}
err := mgr.GetClient().List(context.TODO(), podList, &client.MatchingFields{"indexNodeNameOfPod": node.Name})
```

